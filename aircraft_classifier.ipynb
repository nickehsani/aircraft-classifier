{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from PIL import Image \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingest\n",
    "\n",
    "# Get all labels imported and indexed, with all whitespaces stripped out\n",
    "labels_list = []\n",
    "dataset = []\n",
    "#import dataset to empty arrays of data and label\n",
    "for (root, dirs, files) in os.walk(\"crop/\"):\n",
    "    labels_list = dirs\n",
    "    for dire in dirs:\n",
    "        curr_dir = root + dire\n",
    "        for (rt,di,fi) in os.walk(curr_dir):\n",
    "            for img in fi:\n",
    "                data = {}\n",
    "                data[\"image\"] = T.functional.pil_to_tensor(Image.open(curr_dir+\"/\"+img))#.to(torch.float32)\n",
    "                data[\"label\"] = labels_list.index(dire)\n",
    "                _,h,w = data[\"image\"].shape\n",
    "                if (h <= 1072 and w <= 1760 and h >= 160 and w >= 160):\n",
    "                    dataset.append(data)\n",
    "                # for each file in the sub directory\n",
    "                # append the image and corresponding label to dataset\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10250\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375.85375609756096\n",
      "315.0\n",
      "[191, 166, 207]\n",
      "764.5808780487805\n",
      "708.0\n",
      "[1199]\n",
      "1071\n",
      "1760\n"
     ]
    }
   ],
   "source": [
    "heights = []\n",
    "widths = []\n",
    "for i in dataset:\n",
    "    c,h,w = i[\"image\"].shape\n",
    "    heights.append(h)\n",
    "    widths.append(w)\n",
    "\n",
    "#plt.bar(4000,heights)\n",
    "#plt.show()\n",
    "print (statistics.mean(heights))\n",
    "print (statistics.median(heights))\n",
    "print (statistics.multimode(heights))\n",
    "print (statistics.mean(widths))\n",
    "print (statistics.median(widths))\n",
    "print (statistics.multimode(widths))\n",
    "print (max(heights))\n",
    "print (max(widths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799, 799, 799, 799, 799, 799, 799, 799, 799, 799]\n"
     ]
    }
   ],
   "source": [
    "heights.sort(reverse=True)\n",
    "print(heights[500:510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# height greater than 1504 or widths greater than 3104 is discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height = 1072\n",
    "max_width  = 1760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchwork (nn.Module):\n",
    "    def __init__ (self, patch_size = 16):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (max_height * max_width) // (self.patch_size ** 2)\n",
    "        self.patch_dim = (self.patch_size ** 2) * 3\n",
    "        self.linear_size = self.patch_dim // 2\n",
    "        self.linear_layer = nn.Linear(self.patch_dim,self.linear_size)\n",
    "\n",
    "    def forward(self,images):\n",
    "\n",
    "        positional_embedding = []\n",
    "        for i in range(self.num_patches+1):\n",
    "            \n",
    "            positional_embedding.append([])\n",
    "            for j in range(self.linear_size//2):\n",
    "                j *= 2\n",
    "                positional_embedding[i].append(np.sin(i/(10000 ** (j/self.patch_dim))))\n",
    "                j += 1\n",
    "                positional_embedding[i].append(np.cos(i/(10000 ** (j/self.patch_dim))))\n",
    "            if (self.patch_dim % 2):\n",
    "                j = self.patch_dim - 1\n",
    "                positional_embedding[i].append(np.sin(i/(10000 ** (j/self.patch_dim))))\n",
    "            positional_embedding[i] = torch.as_tensor(positional_embedding[i])\n",
    "        positional_embedding = torch.from_numpy(np.array(positional_embedding))\n",
    "        # positional_embedding.requires_grad=False\n",
    "        all_patches = torch.zeros(len(images),self.num_patches + 1,self.linear_size)\n",
    "        ind=0\n",
    "        for img in images:\n",
    "            \n",
    "            image=img[\"image\"]\n",
    "\n",
    "            image = image.to(torch.float32)\n",
    "            c,h,w = image.shape\n",
    "\n",
    "            #size standardization\n",
    "            patch_height = max_height - h\n",
    "            patch_width = max_width - w\n",
    "\n",
    "            patch_bottom = patch_height // 2\n",
    "            if (patch_height % 2):\n",
    "                patch_top = patch_bottom + 1\n",
    "            else:\n",
    "                patch_top = patch_bottom\n",
    "\n",
    "            patch_right = patch_width // 2\n",
    "\n",
    "            if (patch_width % 2):\n",
    "                patch_left = patch_right + 1\n",
    "            else:\n",
    "                patch_left = patch_right\n",
    "                \n",
    "\n",
    "            padding = torch.nn.ZeroPad2d((patch_left,patch_right,patch_top,patch_bottom))\n",
    "            image = padding(image)\n",
    "            \n",
    "            patches = []\n",
    "                \n",
    "            for i in range(self.num_patches):\n",
    "                x_coor = i // (max_width // self.patch_size)\n",
    "                y_coor = i - x_coor * (max_width // self.patch_size)\n",
    "                patch = image[:, x_coor * self.patch_size: (x_coor + 1) * self.patch_size,y_coor * self.patch_size:(y_coor+1)*self.patch_size]\n",
    "                patches.append(patch.flatten())\n",
    "                \n",
    "            patches = torch.stack(patches)\n",
    "            patches = self.linear_layer(patches)\n",
    "            \n",
    "            classification_token = nn.Parameter(torch.rand(self.linear_size))\n",
    "            patches = torch.vstack((classification_token,patches))\n",
    "            # print(patches.shape)\n",
    "            patches = patches + positional_embedding\n",
    "\n",
    "            all_patches[ind]=patches\n",
    "            ind += 1\n",
    "            gc.collect()\n",
    "        all_patches = torch.as_tensor(all_patches)\n",
    "\n",
    "        return all_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Patchwork()\n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m, in \u001b[0;36mPatchwork.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     73\u001b[0m     all_patches[ind]\u001b[38;5;241m=\u001b[39mpatches\n\u001b[1;32m     74\u001b[0m     ind \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m all_patches \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(all_patches)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_patches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Patchwork()\n",
    "x = model(dataset)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "psudocode for data import \n",
    "    for each .csv file in dataset \n",
    "        get image file using file name, and import label\n",
    "\n",
    "https://github.com/gupta-abhay/pytorch-vit\n",
    "https://github.com/jeonsworld/ViT-pytorch/blob/main/models/modeling_resnet.py\n",
    "https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html\n",
    "https://mrinath.medium.com/vit-part-1-patchify-images-using-pytorch-unfold-716cd4fd4ef6\n",
    "https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "\n",
    "\n",
    "TODO: Get metadata and make a dictionary to translate aircraft types into indexes/usable y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
