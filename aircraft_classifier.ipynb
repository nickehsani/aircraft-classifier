{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from PIL import Image \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingest\n",
    "\n",
    "# Get all labels imported and indexed, with all whitespaces stripped out\n",
    "labels_list = []\n",
    "dataset = []\n",
    "#import dataset to empty arrays of data and label\n",
    "for (root, dirs, files) in os.walk(\"crop/\"):\n",
    "    labels_list = dirs\n",
    "    for dire in dirs:\n",
    "        curr_dir = root + dire\n",
    "        for (rt,di,fi) in os.walk(curr_dir):\n",
    "            for img in fi:\n",
    "                data = {}\n",
    "                data[\"image\"] = T.functional.pil_to_tensor(Image.open(curr_dir+\"/\"+img))#.to(torch.float32)\n",
    "                data[\"label\"] = labels_list.index(dire)\n",
    "                dataset.append(data)\n",
    "                # for each file in the sub directory\n",
    "                # append the image and corresponding label to dataset\n",
    "    break\n",
    "\n",
    "random.shuffle(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6335\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330.17537490134174\n",
      "212\n",
      "[71]\n",
      "684.1434885556432\n",
      "479\n",
      "[1199]\n",
      "3857\n",
      "6876\n"
     ]
    }
   ],
   "source": [
    "heights = []\n",
    "widths = []\n",
    "for i in dataset:\n",
    "    c,h,w = i[\"image\"].shape\n",
    "    heights.append(h)\n",
    "    widths.append(w)\n",
    "\n",
    "#plt.bar(4000,heights)\n",
    "#plt.show()\n",
    "print (statistics.mean(heights))\n",
    "print (statistics.median(heights))\n",
    "print (statistics.multimode(heights))\n",
    "print (statistics.mean(widths))\n",
    "print (statistics.median(widths))\n",
    "print (statistics.multimode(widths))\n",
    "print (max(heights))\n",
    "print (max(widths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height = 4912\n",
    "max_width  = 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchwork ():\n",
    "    def __init__ (self, patch_size = 16):\n",
    "        # super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def patch(self,image):\n",
    "        image = image.to(torch.float32)\n",
    "        c,h,w = image.shape\n",
    "\n",
    "        #size standardization\n",
    "        patch_height = max_height - h\n",
    "        patch_width = max_width - w\n",
    "\n",
    "        patch_bottom = patch_height / 2\n",
    "        if (patch_height % 2):\n",
    "            patch_top = patch_bottom + 1\n",
    "        else:\n",
    "            patch_top = patch_bottom\n",
    "\n",
    "        patch_right = patch_width / 2\n",
    "\n",
    "        if (patch_width % 2):\n",
    "            patch_left = patch_right + 1\n",
    "        else:\n",
    "            patch_left = patch_right\n",
    "\n",
    "        padding = torch.nn.ZeroPad2d((patch_left,patch_right,patch_top,patch_bottom))\n",
    "        image = padding(image)\n",
    "        \n",
    "        patch_dim = (self.patch_size) ** 2\n",
    "        num_patches = patch_height * patch_width / patch_dim\n",
    "        patches = []\n",
    "\n",
    "        for i in range(num_patches):\n",
    "            x_coor = i % (patch_width / patch_dim)\n",
    "            y_coor = i - x_coor * patch_width / patch_dim\n",
    "\n",
    "            patches.append(image[:, x_coor * self.patch_size: (x_coor + 1) * self.patch_size,y_coor * self.patch_size:(j+1)*self.patch_size].flatten())\n",
    "\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072, 104])\n"
     ]
    }
   ],
   "source": [
    "model = Patchwork()\n",
    "print(model(dataset[69][\"image\"]).shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "psudocode for data import \n",
    "    for each .csv file in dataset \n",
    "        get image file using file name, and import label\n",
    "\n",
    "https://github.com/gupta-abhay/pytorch-vit\n",
    "https://github.com/jeonsworld/ViT-pytorch/blob/main/models/modeling_resnet.py\n",
    "https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html\n",
    "https://mrinath.medium.com/vit-part-1-patchify-images-using-pytorch-unfold-716cd4fd4ef6\n",
    "https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "\n",
    "\n",
    "TODO: Get metadata and make a dictionary to translate aircraft types into indexes/usable y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4191691561.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Transformer (nn.Module):\n",
    "    def __init__(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
